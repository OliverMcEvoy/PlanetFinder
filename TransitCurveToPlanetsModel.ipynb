{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, iteration):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.iteration = iteration\n",
    "        self.file = h5py.File(hdf5_file, 'r')\n",
    "        self.systems = list(self.file[iteration].keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.systems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        system = self.systems[idx]\n",
    "        group = self.file[f'{self.iteration}/{system}']\n",
    "        combined_light_curve = torch.tensor(group['combined_light_curve'][:], dtype=torch.float32)\n",
    "        detected_count = torch.tensor(group['detected_count'][()], dtype=torch.float32)\n",
    "        return combined_light_curve, detected_count\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(PyroModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = PyroModule[nn.Linear](1000, 128)  # Adjust input size as needed\n",
    "        self.fc1.weight = PyroSample(dist.Normal(0., 1.).expand([128, 1000]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(0., 1.).expand([128]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroModule[nn.Linear](128, 64)\n",
    "        self.fc2.weight = PyroSample(dist.Normal(0., 1.).expand([64, 128]).to_event(2))\n",
    "        self.fc2.bias = PyroSample(dist.Normal(0., 1.).expand([64]).to_event(1))\n",
    "        \n",
    "        self.fc3 = PyroModule[nn.Linear](64, 1)\n",
    "        self.fc3.weight = PyroSample(dist.Normal(0., 1.).expand([1, 64]).to_event(2))\n",
    "        self.fc3.bias = PyroSample(dist.Normal(0., 1.).expand([1]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc3(x).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset and dataloader\n",
    "dataset = PlanetDataset('planet_systems.h5', 'iteration_0')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model and guide\n",
    "bnn = BNN().cuda()  # Move the model to GPU\n",
    "guide = pyro.infer.autoguide.AutoDiagonalNormal(bnn)\n",
    "\n",
    "# Define the optimizer and SVI object\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(bnn, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        light_curves, counts = batch\n",
    "        light_curves = light_curves.cuda()  # Move data to GPU\n",
    "        counts = counts.cuda()  # Move data to GPU\n",
    "        epoch_loss += svi.step(light_curves, counts)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check code \n",
    "with h5py.File('planet_systems.hdf5', 'r') as hdf5_file:\n",
    "      # Function to recursively print the structure of the HDF5 file\n",
    "      def print_structure(name, obj):\n",
    "            print(name)\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                  for key, value in obj.items():\n",
    "                        print_structure(f\"{name}/{key}\", value)\n",
    "            elif isinstance(obj, h5py.Dataset):\n",
    "                  print(f\"  Dataset: {name}, shape: {obj.shape}, dtype: {obj.dtype}\")\n",
    "\n",
    "      # Extract and print some sample values\n",
    "      iteration = 'iteration_0'\n",
    "      system = 'system_0'\n",
    "      planet = 'planet_0'\n",
    "\n",
    "\n",
    "      print(\"\\nExtracting sample values for iteration_0/system_0:\")\n",
    "      print(\"Combined light curve:\", hdf5_file[f'{iteration}/{system}/combined_light_curve'][:5])  # Print first 5 values\n",
    "      print(\"Detected count:\", hdf5_file[f'{iteration}/{system}/detected_count'][()])\n",
    "      print(\"Flux with noise:\", hdf5_file[f'{iteration}/{system}/flux_with_noise'][:5])  # Print first 5 values\n",
    "      print(\"Observation noise:\", hdf5_file[f'{iteration}/{system}/observation_noise'][()])\n",
    "      print(\"Star radius:\", hdf5_file[f'{iteration}/{system}/star_radius'][()])\n",
    "      print(\"Total time:\", hdf5_file[f'{iteration}/{system}/total_time'][()])\n",
    "      print(\"u1:\", hdf5_file[f'{iteration}/{system}/u1'][()])\n",
    "      print(\"u2:\", hdf5_file[f'{iteration}/{system}/u2'][()])\n",
    "      print(\"Time:\", hdf5_file[f'{iteration}/{system}/time'][:5])  # Print first 5 values\n",
    "      \n",
    "      print(\"\\nExtracting sample values for iteration_0/system_0/planet_0:\")\n",
    "      print(\"a:\", hdf5_file[f'{iteration}/{system}/planets/{planet}/a'][()])\n",
    "      print(\"incl:\", hdf5_file[f'{iteration}/{system}/planets/{planet}/incl'][()])\n",
    "      print(\"period:\", hdf5_file[f'{iteration}/{system}/planets/{planet}/period'][()])\n",
    "      print(\"rp:\", hdf5_file[f'{iteration}/{system}/planets/{planet}/rp'][()])\n",
    "      print(\"transit_midpoint:\", hdf5_file[f'{iteration}/{system}/planets/{planet}/transit_midpoint'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
